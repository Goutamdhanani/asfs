model:
  # GitHub Models endpoint (OpenAI-compatible)
  endpoint: "https://models.inference.ai.azure.com"
  
  # Model name (available on GitHub Models)
  # Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
  model_name: "gpt-4o"
  
  # API key from environment variable
  api_key: null  # Set via GITHUB_TOKEN env var
  
  # Generation parameters
  temperature: 0.2  # Lowered from 0.7 - reduces variance, stabilizes output
  max_tokens: 1024
  
  # Scoring configuration
  max_segments_to_score: 50
  min_score_threshold: 6.0
  
  # Rate limiting and batching
  batch_size: 6  # Segments per API call
  pre_filter_count: 20  # Max candidates after heuristic filtering
  inter_request_delay: 1.5  # Seconds between API calls
  max_cooldown_threshold: 60  # If retry-after > this, stop pipeline
  
  # LLM Backend Selection (NEW)
  # Options: "auto" | "local" | "api"
  # - auto (default): Try local Ollama first, fallback to API if unavailable
  # - local: Require Ollama, fallback to API with warning if unavailable
  # - api: Use API only, skip local entirely
  llm_backend: "auto"
  
  # Local LLM Configuration (Ollama)
  local_model_name: "qwen2.5:3b-instruct"  # Supported: qwen2.5:3b-instruct, qwen2.5:7b-instruct
  local_endpoint: "http://localhost:11434"
  
  # Ollama-specific settings
  local_keep_alive: "5m"  # How long to keep model in memory (5m, 10m, -1 for indefinite)
  
  # Circuit breaker configuration
  circuit_breaker_threshold: 3  # Number of consecutive local failures before switching to API
