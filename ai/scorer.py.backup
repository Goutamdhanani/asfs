"""AI highlight scoring using GitHub Models API."""

import os
import json
import re
import logging
import time
import random
import requests
from typing import List, Dict
from pathlib import Path

logger = logging.getLogger(__name__)

# Try importing Azure AI Inference SDK
try:
    from azure.ai.inference import ChatCompletionsClient
    from azure.core.credentials import AzureKeyCredential
    AZURE_SDK_AVAILABLE = True
except ImportError:
    AZURE_SDK_AVAILABLE = False
    logger.warning("Azure AI Inference SDK not available, will try OpenAI SDK")

# Fallback to OpenAI SDK
try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False

# Ollama removed - using GitHub Models API only
OLLAMA_AVAILABLE = False


def extract_json_safe(text: str) -> dict:
    """
    Safely extract and parse JSON from model response.
    
    Handles multiple response formats from different LLM providers:
    - Clean JSON: {"hook_score": 7, ...}
    - Markdown wrapped: ```json\n{...}\n```
    - Text with JSON: "Here's the score: {...}"
    - Newline prefixed: \n  {...}
    
    Args:
        text: Raw model response string
        
    Returns:
        Parsed dictionary with scores
        
    Raises:
        ValueError: If no valid JSON found
    """
    if not text or not isinstance(text, str):
        raise ValueError("Empty or invalid response")
    
    # Remove markdown code blocks
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    
    # Try to find JSON object using brace counting for accuracy
    # This ensures we get complete nested objects
    start_idx = text.find('{')
    if start_idx == -1:
        raise ValueError(f"No JSON object found in response: {text[:200]}")
    
    # Count braces to find matching closing brace
    brace_count = 0
    end_idx = start_idx
    
    for i in range(start_idx, len(text)):
        if text[i] == '{':
            brace_count += 1
        elif text[i] == '}':
            brace_count -= 1
            if brace_count == 0:
                end_idx = i + 1
                break
    
    if brace_count != 0:
        # Fallback: try regex for simpler cases
        match = re.search(r'\{[^{}]*\}', text)
        if match:
            json_str = match.group()
        else:
            raise ValueError(f"Unbalanced braces in JSON: {text[start_idx:start_idx+200]}")
    else:
        json_str = text[start_idx:end_idx]
    
    try:
        data = json.loads(json_str)
        return data
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON: {e}\nExtracted: {json_str[:200]}")


def extract_score_safe(data: dict, field: str, default: float = 0.0) -> float:
    """
    Safely extract a numeric score from parsed JSON.
    
    Handles multiple schema variations:
    - Direct key: {"hook_score": 7}
    - Nested: {"scores": {"hook_score": 7}}
    - String numbers: {"hook_score": "7"}
    
    Individual scores are expected to be on a 0-10 scale.
    The final_score field is expected to be on a 0-100 scale.
    
    Args:
        data: Parsed JSON dictionary
        field: Field name to extract (e.g., "hook_score")
        default: Default value if field not found (can be None for optional fields)
        
    Returns:
        Numeric score as float, or default if not found/invalid
    """
    try:
        # Try direct access
        if field in data:
            value = data[field]
            # Return None explicitly if the value is None and default is None
            if value is None and default is None:
                return None
            return float(value)
        
        # Try nested in "scores"
        if "scores" in data and isinstance(data["scores"], dict):
            if field in data["scores"]:
                return float(data["scores"][field])
        
        # Try without underscore (e.g., "hookscore")
        alt_field = field.replace("_", "")
        if alt_field in data:
            return float(data[alt_field])
        
        # Not found - return default
        return default
        
    except (ValueError, TypeError):
        return default if default is not None else 0.0



def load_prompt_template() -> str:
    """Load the scoring prompt template."""
    prompt_path = Path(__file__).parent / "prompt.txt"
    
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt template not found: {prompt_path}")
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()


# Ollama functions removed - using GitHub Models API only


def create_batch_prompt(segments: List[Dict], prompt_template: str) -> str:
    """
    Create a batch scoring prompt for multiple segments.
    
    Args:
        segments: List of segments to score together (batch size is configurable)
        prompt_template: Base template
        
    Returns:
        Formatted batch prompt
    """
    batch_data = []
    for i, seg in enumerate(segments):
        batch_data.append({
            "id": i + 1,
            "text": seg["text"],
            "duration": seg["duration"]
        })
    
    batch_prompt = f"""
Score the following {len(segments)} video segments. Return a JSON array with results for each.

Segments:
{json.dumps(batch_data, indent=2)}

{prompt_template}

Return format:
{{
  "results": [
    {{
      "id": 1,
      "hook_score": 7,
      "retention_score": 6,
      "emotion_score": 5,
      "relatability_score": 4,
      "completion_score": 6,
      "platform_fit_score": 5,
      "final_score": 60,
      "verdict": "maybe",
      "key_strengths": ["strength 1", "strength 2"],
      "key_weaknesses": ["weakness 1"],
      "first_3_seconds": "exact quote from first 3 seconds",
      "primary_emotion": "neutral",
      "optimal_platform": "tiktok"
    }},
    ...
  ]
}}
"""
    return batch_prompt


def pre_filter_candidates(candidates: List[Dict], max_count: int = 20) -> List[Dict]:
    """
    Pre-filter candidates using heuristics before AI scoring.
    
    Criteria:
    - Duration: 20-60 seconds
    - High pause density (natural breaks)
    - Emotional keywords
    - Sentence density (engagement)
    
    Args:
        candidates: All candidate segments
        max_count: Maximum to return
        
    Returns:
        Filtered candidates most likely to be viral
    """
    EMOTIONAL_KEYWORDS = [
        'never', 'always', 'nobody', 'everyone', 'shocked', 'crazy', 
        'insane', 'ruined', 'destroyed', 'unbelievable', 'secret', 
        'truth', 'lie', 'wrong', 'right', 'mistake', 'regret'
    ]
    
    scored_candidates = []
    
    for candidate in candidates:
        heuristic_score = 0.0
        text = candidate.get('text', '').lower()
        duration = candidate.get('duration', 0)
        
        # Duration check (20-60s ideal)
        if 20 <= duration <= 60:
            heuristic_score += 3.0
        elif 15 <= duration <= 75:
            heuristic_score += 1.5
        
        # Emotional keywords
        keyword_count = sum(1 for kw in EMOTIONAL_KEYWORDS if kw in text)
        heuristic_score += min(keyword_count * 0.5, 3.0)
        
        # Sentence density (engagement indicator)
        sentences = text.count('.') + text.count('!') + text.count('?')
        if duration > 0:
            sentence_density = sentences / (duration / 10)  # Sentences per 10s
            heuristic_score += min(sentence_density * 0.8, 2.0)
        
        # Pause density (from metadata if available)
        pause_density = candidate.get('pause_density', 0)
        heuristic_score += min(pause_density * 2.0, 2.0)
        
        scored_candidates.append({
            **candidate,
            'heuristic_score': heuristic_score
        })
    
    # Sort by heuristic score and return top N
    scored_candidates.sort(key=lambda x: x['heuristic_score'], reverse=True)
    return scored_candidates[:max_count]


def save_pipeline_state(scored_segments: List[Dict], remaining_segments: List[Dict], output_path: str = "state/pipeline_state.json"):
    """Save pipeline state when rate limited."""
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    state = {
        'timestamp': time.time(),
        'scored_segments': scored_segments,
        'remaining_segments': remaining_segments,
        'reason': 'rate_limit_exceeded'
    }
    
    with open(output_path, 'w') as f:
        json.dump(state, f, indent=2)
    
    logger.info(f"Pipeline state saved to {output_path}")


def extract_retry_after(api_error) -> int:
    """
    Extract retry-after value from API error response.
    
    Args:
        api_error: Exception from API call
        
    Returns:
        Retry-after value in seconds, or None if not found
    """
    retry_after = None
    
    # Try to get from response headers (standard HTTP header)
    if hasattr(api_error, 'response') and hasattr(api_error.response, 'headers'):
        retry_after_header = api_error.response.headers.get('retry-after') or \
                           api_error.response.headers.get('Retry-After')
        if retry_after_header:
            try:
                retry_after = int(retry_after_header)
            except (ValueError, TypeError):
                pass
    
    # Fallback: try direct attribute (some SDKs may expose it directly)
    if retry_after is None:
        retry_after = getattr(api_error, 'retry_after', None)
    
    return retry_after


def process_single_segment_response(segment: Dict, ai_analysis: Dict, idx: int) -> Dict:
    """
    Process AI analysis for a single segment and create scored segment.
    
    Args:
        segment: Original segment data
        ai_analysis: Parsed AI response
        idx: Segment index for logging
        
    Returns:
        Scored segment dictionary
    """
    try:
        # Extract scores safely with fallbacks
        hook_score = extract_score_safe(ai_analysis, "hook_score", 0.0)
        retention_score = extract_score_safe(ai_analysis, "retention_score", 0.0)
        emotion_score = extract_score_safe(ai_analysis, "emotion_score", 0.0)
        relatability_score = extract_score_safe(ai_analysis, "relatability_score", 0.0)
        completion_score = extract_score_safe(ai_analysis, "completion_score", 0.0)
        platform_fit_score = extract_score_safe(ai_analysis, "platform_fit_score", 0.0)
        
        # Extract final score with fallback calculation
        final_score = extract_score_safe(ai_analysis, "final_score", None)
        
        # If final_score not provided, calculate weighted average
        if final_score is None:
            final_score = (
                hook_score * 0.35 +
                retention_score * 0.25 +
                emotion_score * 0.20 +
                completion_score * 0.10 +
                platform_fit_score * 0.05 +
                relatability_score * 0.05
            ) * 10.0  # Scale from 0-10 to 0-100
        
        # Convert final_score (0-100) to overall_score (0-10)
        overall_score = final_score / 10.0
        verdict = ai_analysis.get("verdict", "skip")
        
        scored_segment = {
            **segment,
            "ai_analysis": ai_analysis,
            "overall_score": overall_score,
            "hook_score": hook_score,
            "retention_score": retention_score,
            "emotion_score": emotion_score,
            "relatability_score": relatability_score,
            "completion_score": completion_score,
            "platform_fit_score": platform_fit_score,
            "final_score": final_score,
            "verdict": verdict,
            "key_strengths": ai_analysis.get("key_strengths", []),
            "key_weaknesses": ai_analysis.get("key_weaknesses", []),
            "first_3_seconds": ai_analysis.get("first_3_seconds", ""),
            "primary_emotion": ai_analysis.get("primary_emotion", "neutral"),
            "optimal_platform": ai_analysis.get("optimal_platform", "none")
        }
        
        logger.info(f"[OK] Segment {idx + 1}: final_score={final_score:.1f}/100, verdict={verdict}")
        return scored_segment
        
    except Exception as e:
        logger.error(f"Failed to process segment {idx + 1} response: {e}")
        return create_fallback_segment(segment)


def create_fallback_segment(segment: Dict) -> Dict:
    """
    Create a fallback segment with zero scores when AI analysis fails.
    
    Args:
        segment: Original segment data
        
    Returns:
        Segment with default scores
    """
    return {
        **segment,
        "ai_analysis": {
            "error": "AI analysis failed",
            "final_score": 0
        },
        "overall_score": 0.0,
        "final_score": 0,
        "verdict": "skip",
        "hook_score": 0,
        "retention_score": 0,
        "emotion_score": 0,
        "relatability_score": 0,
        "completion_score": 0,
        "platform_fit_score": 0,
        "key_strengths": [],
        "key_weaknesses": ["AI analysis failed"],
        "first_3_seconds": "",
        "primary_emotion": "neutral",
        "optimal_platform": "none"
    }


def score_segments(
    candidates: List[Dict],
    model_config: Dict,
    max_segments: int = 50
) -> List[Dict]:
    """
    Score candidate segments using GitHub Models API.
    
    Args:
        candidates: List of candidate segment dictionaries
        model_config: Model configuration (endpoint, model_name, api_key)
        max_segments: Maximum number of segments to score (top candidates first)
        
    Returns:
        List of scored segments with AI analysis
    """
    if not candidates:
        logger.warning("No candidate segments to score")
        return []
    
    # Get API credentials from config or environment
    endpoint = model_config.get("endpoint") or os.getenv("GITHUB_MODELS_ENDPOINT")
    api_key = model_config.get("api_key") or os.getenv("GITHUB_TOKEN")
    model_name = model_config.get("model_name", "gpt-4o")
    
    if not endpoint:
        endpoint = "https://models.inference.ai.azure.com"
    
    if not api_key:
        raise ValueError("GitHub API token not provided. Set GITHUB_TOKEN environment variable.")
    
    logger.info(f"Scoring {min(len(candidates), max_segments)} segments using {model_name}")
    
    # Load prompt template
    prompt_template = load_prompt_template()
    
    # Backend selection configuration
    llm_backend = model_config.get("llm_backend", "auto")  # auto | local | api
    local_model_name = model_config.get("local_model_name", "qwen3:latest")
    local_endpoint = model_config.get("local_endpoint", "http://localhost:11434")
    temperature = model_config.get("temperature", 0.2)  # Use from config
    local_keep_alive = model_config.get("local_keep_alive", "5m")  # Memory management
    
    # Determine if we should try local model
    ollama_available = False
    exact_model_name = None  # Store exact matched model name
    
    if llm_backend == "api":
        logger.info("Backend: API only (local disabled by config)")
    elif llm_backend in ["auto", "local"]:
        # Check Ollama availability - returns (bool, exact_model_name)
        ollama_available, exact_model_name = check_ollama_availability(local_model_name, endpoint=local_endpoint)
        
        if ollama_available:
            logger.info(f"Ollama is running with model: {exact_model_name}")
        else:
            if llm_backend == "local":
                logger.warning("Backend: LOCAL required but Ollama unavailable - falling back to API")
            else:  # auto
                logger.info("Backend: AUTO - Ollama unavailable, using API")
    else:
        logger.warning(f"Unknown llm_backend '{llm_backend}', defaulting to 'auto'")
        ollama_available, exact_model_name = check_ollama_availability(local_model_name, endpoint=local_endpoint)
        if ollama_available:
            logger.info(f"Ollama is running with model: {exact_model_name}")
        else:
            logger.info("Backend: AUTO - Ollama unavailable, using API")
    
    # Pre-filter candidates using heuristics
    pre_filter_count = model_config.get('pre_filter_count', 20)
    logger.info(f"Pre-filtering {len(candidates)} candidates using heuristics")
    candidates = pre_filter_candidates(candidates, max_count=pre_filter_count)
    logger.info(f"After pre-filtering: {len(candidates)} candidates remain")
    
    # Limit to max_segments (score best candidates based on duration and position)
    segments_to_score = candidates[:max_segments] if len(candidates) > max_segments else candidates
    
    scored_segments = []
    
    # Determine effective batch size based on backend
    # Ollama doesn't support batching, so we force single-segment processing
    BATCH_SIZE = model_config.get('batch_size', 6)  # Get configured batch size
    
    if ollama_available:
        effective_batch_size = 1  # Force single-segment processing for Ollama
        logger.info("[LOCAL] Ollama processes one segment at a time (no batching)")
        logger.info(f"Backend: Local LLM (Ollama)")
        logger.info(f"Will score {len(segments_to_score)} segments using LOCAL Ollama with keep_alive={local_keep_alive}")
    else:
        effective_batch_size = BATCH_SIZE
        logger.info(f"[API] Batch processing enabled (batch_size={effective_batch_size})")
        logger.info(f"Will score {len(segments_to_score)} segments using API ({effective_batch_size} per batch)")
    
    # Initialize AI client only when needed (not for pure local mode)
    client = None
    
    # Only initialize API client if we're NOT in pure local mode OR if in auto mode (for fallback)
    should_init_api_client = (
        (llm_backend == "api") or 
        (llm_backend == "auto" and not ollama_available)
    )
    
    # Special case: local mode requested but Ollama unavailable
    if llm_backend == "local" and not ollama_available:
        logger.error("Local LLM mode requested but Ollama is unavailable")
        logger.error("Please ensure Ollama is running and model is available")
        logger.info("To use API instead, change llm_backend to 'auto' or 'api' in config/model.yaml")
        raise RuntimeError("Local LLM mode requested but Ollama unavailable. Set llm_backend='auto' for fallback.")
    
    if should_init_api_client:
        if AZURE_SDK_AVAILABLE:
            try:
                client = ChatCompletionsClient(
                    endpoint=endpoint,
                    credential=AzureKeyCredential(api_key)
                )
                logger.info("Using Azure AI Inference SDK")
            except Exception as e:
                logger.warning(f"Failed to initialize Azure SDK: {e}")
        
        if client is None and OPENAI_SDK_AVAILABLE:
            try:
                client = OpenAI(
                    base_url=endpoint,
                    api_key=api_key
                )
                logger.info("Using OpenAI SDK")
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAI SDK: {e}")
        
        if client is None:
            if ollama_available and llm_backend == "auto":
                # Fallback to local is OK in auto mode
                logger.info("API client initialization failed, will use local Ollama")
            else:
                raise RuntimeError("No compatible SDK available. Install azure-ai-inference or openai.")
    
    # Define system message once (used for all segments)
    system_message = """You are a video content analyzer. 

CRITICAL: You MUST respond with ONLY valid JSON. 
- Do NOT include markdown code blocks
- Do NOT include any text before or after the JSON
- Start directly with { and end with }"""
    
    # API usage tracking
    api_calls_made = 0
    tokens_used = 0
    
    # Circuit breaker for local LLM failures (Issue 7)
    CIRCUIT_BREAKER_THRESHOLD = model_config.get('circuit_breaker_threshold', 3)
    local_failure_count = 0
    consecutive_memory_errors = 0
    MAX_MEMORY_ERRORS = 3
    
    # Get other config
    inter_request_delay = model_config.get('inter_request_delay', 1.5)
    max_cooldown_threshold = model_config.get('max_cooldown_threshold', 60)
    
    # Process segments with effective batch size
    for batch_start in range(0, len(segments_to_score), effective_batch_size):
        batch_end = min(batch_start + effective_batch_size, len(segments_to_score))
        batch_segments = segments_to_score[batch_start:batch_end]
        
        # Separate Ollama path from API path
        if ollama_available and local_failure_count < CIRCUIT_BREAKER_THRESHOLD:
            # Process with Ollama (one segment at a time)
            for i, segment in enumerate(batch_segments):
                idx = batch_start + i
                
                try:
                    # Format prompt with segment data using token replacement
                    prompt = (
                        prompt_template
                        .replace("{{SEGMENT_TEXT}}", segment["text"])
                        .replace("{{DURATION}}", f"{segment['duration']:.1f}")
                    )
                    
                    ai_analysis = None
                    
                    # Try local model
                    try:
                        ai_analysis = score_with_ollama(
                            segment=segment,
                            prompt=prompt,
                            model_name=exact_model_name,  # Use exact matched model name
                            temperature=temperature,
                            endpoint=local_endpoint,
                            keep_alive=local_keep_alive  # Memory management
                        )
                        logger.debug(f"Segment {idx + 1}/{len(segments_to_score)}: scored with LOCAL Ollama [OK]")
                        
                        # Reset failure counters on success
                        local_failure_count = 0
                        consecutive_memory_errors = 0
                        
                    except Exception as local_error:
                        local_failure_count += 1
                        error_str = str(local_error).lower()
                        
                        # Check for memory errors
                        if "memory" in error_str or "allocation" in error_str:
                            consecutive_memory_errors += 1
                            logger.warning(f"Local LLM memory error ({consecutive_memory_errors}/{MAX_MEMORY_ERRORS}): {local_error}")
                            
                            if consecutive_memory_errors >= MAX_MEMORY_ERRORS:
                                logger.error("[CIRCUIT BREAKER] Persistent memory errors - disabling local LLM")
                                logger.info("[TIP] Use CPU mode: set OLLAMA_NO_GPU=1 or try smaller model")
                                ollama_available = False
                                local_failure_count = CIRCUIT_BREAKER_THRESHOLD  # Force circuit break
                        else:
                            logger.warning(f"Local LLM failed ({local_failure_count}/{CIRCUIT_BREAKER_THRESHOLD}): {local_error}")
                        
                        # Check circuit breaker threshold
                        if local_failure_count >= CIRCUIT_BREAKER_THRESHOLD:
                            logger.error(f"[CIRCUIT BREAKER] Disabling local LLM after {local_failure_count} failures")
                            ollama_available = False
                        
                        # Fallback to API if auto mode and client available
                        if llm_backend == "auto" and client is not None:
                            logger.info("Falling back to remote API")
                            ai_response = None
                            api_call_succeeded = False
                            max_retries = 3
                            
                            for attempt in range(max_retries + 1):
                                try:
                                    if AZURE_SDK_AVAILABLE and isinstance(client, ChatCompletionsClient):
                                        response = client.complete(
                                            messages=[
                                                {"role": "system", "content": system_message},
                                                {"role": "user", "content": prompt}
                                            ],
                                            model=model_name,
                                            temperature=temperature,
                                            max_tokens=1024,
                                            response_format={"type": "json_object"}
                                        )
                                        ai_response = response.choices[0].message.content
                                        
                                        # Track token usage
                                        if hasattr(response, 'usage'):
                                            tokens_used += response.usage.total_tokens
                                        
                                    else:  # OpenAI SDK
                                        response = client.chat.completions.create(
                                            model=model_name,
                                            messages=[
                                                {"role": "system", "content": system_message},
                                                {"role": "user", "content": prompt}
                                            ],
                                            temperature=temperature,
                                            max_tokens=1024,
                                            response_format={"type": "json_object"}
                                        )
                                        ai_response = response.choices[0].message.content
                                        
                                        # Track token usage
                                        if hasattr(response, 'usage'):
                                            tokens_used += response.usage.total_tokens
                                    
                                    api_call_succeeded = True
                                    break
                                    
                                except Exception as api_error:
                                    # Check for rate limiting (429)
                                    is_rate_limit = False
                                    retry_after = None
                                    
                                    if hasattr(api_error, 'status_code') and api_error.status_code == 429:
                                        is_rate_limit = True
                                        # Try to extract retry-after
                                        if hasattr(api_error, 'response') and hasattr(api_error.response, 'headers'):
                                            retry_after = api_error.response.headers.get('retry-after')
                                            if retry_after:
                                                retry_after = int(retry_after)
                                    
                                    if is_rate_limit and retry_after and retry_after > 60:
                                        logger.error(f"Rate limit with long cooldown ({retry_after}s). Stopping.")
                                        raise RuntimeError(f"Rate limit exceeded: retry after {retry_after}s")
                                    
                                    if attempt < max_retries:
                                        # Exponential backoff with jitter
                                        if is_rate_limit and retry_after:
                                            backoff = retry_after + random.uniform(1, 5)
                                        else:
                                            backoff = min(60, (2 ** attempt) + random.uniform(0, 1))
                                        
                                        logger.warning(f"API call failed for segment {idx + 1}, retrying in {backoff:.1f}s... ({api_error})")
                                        time.sleep(backoff)
                                    else:
                                        logger.error(f"API call failed for segment {idx + 1} after {max_retries + 1} attempts: {api_error}")
                                        raise
                            
                            if api_call_succeeded and ai_response is not None:
                                # Parse API response
                                try:
                                    ai_analysis = extract_json_safe(ai_response)
                                    logger.debug(f"Segment {idx + 1}: scored with API (fallback)")
                                except ValueError as e:
                                    logger.warning(f"Failed to parse API response for segment {idx + 1}: {e}")
                                    raise
                        else:
                            # No fallback available
                            raise
                    
                    # Process the segment response
                    if ai_analysis is not None:
                        scored_segment = process_single_segment_response(segment, ai_analysis, idx)
                        scored_segments.append(scored_segment)
                    else:
                        scored_segments.append(create_fallback_segment(segment))
                    
                except Exception as e:
                    logger.error(f"Failed to score segment {idx + 1}: {str(e)}")
                    scored_segments.append(create_fallback_segment(segment))
        
        else:
            # Process with API (batched or when local unavailable/circuit broken)
            # Initialize API client on-demand if not yet initialized (for circuit breaker case)
            if client is None:
                logger.info("Initializing API client for fallback...")
                if AZURE_SDK_AVAILABLE:
                    try:
                        client = ChatCompletionsClient(
                            endpoint=endpoint,
                            credential=AzureKeyCredential(api_key)
                        )
                        logger.info("Using Azure AI Inference SDK (on-demand)")
                    except Exception as e:
                        logger.error(f"Failed to initialize Azure SDK: {e}")
                
                if client is None and OPENAI_SDK_AVAILABLE:
                    try:
                        client = OpenAI(
                            base_url=endpoint,
                            api_key=api_key
                        )
                        logger.info("Using OpenAI SDK (on-demand)")
                    except Exception as e:
                        logger.error(f"Failed to initialize OpenAI SDK: {e}")
                
                if client is None:
                    logger.error("Cannot initialize API client for fallback - no compatible SDK available")
                    # Continue with fallback segments for remaining items
                    for i, segment in enumerate(batch_segments):
                        idx = batch_start + i
                        scored_segments.append(create_fallback_segment(segment))
                    continue
            
            if len(batch_segments) == 1:
                # Single segment API processing
                segment = batch_segments[0]
                idx = batch_start
                
                try:
                    # Format prompt with segment data using token replacement
                    prompt = (
                        prompt_template
                        .replace("{{SEGMENT_TEXT}}", segment["text"])
                        .replace("{{DURATION}}", f"{segment['duration']:.1f}")
                    )
                    
                    ai_response = None
                    api_call_succeeded = False
                    max_retries = 3
                    
                    for attempt in range(max_retries + 1):
                        try:
                            if AZURE_SDK_AVAILABLE and isinstance(client, ChatCompletionsClient):
                                response = client.complete(
                                    messages=[
                                        {"role": "system", "content": system_message},
                                        {"role": "user", "content": prompt}
                                    ],
                                    model=model_name,
                                    temperature=temperature,
                                    max_tokens=1024,
                                    response_format={"type": "json_object"}
                                )
                                ai_response = response.choices[0].message.content
                                
                                # Track token usage
                                if hasattr(response, 'usage'):
                                    tokens_used += response.usage.total_tokens
                                
                            else:  # OpenAI SDK
                                response = client.chat.completions.create(
                                    model=model_name,
                                    messages=[
                                        {"role": "system", "content": system_message},
                                        {"role": "user", "content": prompt}
                                    ],
                                    temperature=temperature,
                                    max_tokens=1024,
                                    response_format={"type": "json_object"}
                                )
                                ai_response = response.choices[0].message.content
                                
                                # Track token usage
                                if hasattr(response, 'usage'):
                                    tokens_used += response.usage.total_tokens
                            
                            api_call_succeeded = True
                            break
                            
                        except Exception as api_error:
                            # Check for rate limiting (429)
                            is_rate_limit = False
                            retry_after = None
                            
                            if hasattr(api_error, 'status_code') and api_error.status_code == 429:
                                is_rate_limit = True
                                # Try to extract retry-after
                                if hasattr(api_error, 'response') and hasattr(api_error.response, 'headers'):
                                    retry_after = api_error.response.headers.get('retry-after')
                                    if retry_after:
                                        retry_after = int(retry_after)
                            
                            if is_rate_limit and retry_after and retry_after > 60:
                                logger.error(f"Rate limit with long cooldown ({retry_after}s). Stopping.")
                                raise RuntimeError(f"Rate limit exceeded: retry after {retry_after}s")
                            
                            if attempt < max_retries:
                                # Exponential backoff with jitter
                                if is_rate_limit and retry_after:
                                    backoff = retry_after + random.uniform(1, 5)
                                else:
                                    backoff = min(60, (2 ** attempt) + random.uniform(0, 1))
                                
                                logger.warning(f"API call failed for segment {idx + 1}, retrying in {backoff:.1f}s... ({api_error})")
                                time.sleep(backoff)
                            else:
                                logger.error(f"API call failed for segment {idx + 1} after {max_retries + 1} attempts: {api_error}")
                                raise
                    
                    if not api_call_succeeded or ai_response is None:
                        raise RuntimeError("API call failed and no response received")
                    
                    # Parse API response
                    try:
                        ai_analysis = extract_json_safe(ai_response)
                        logger.debug(f"Segment {idx + 1}/{len(segments_to_score)}: scored with API")
                    except ValueError as e:
                        logger.warning(f"Failed to parse API response for segment {idx + 1}: {e}")
                        raise
                    
                    # Process single segment response
                    scored_segment = process_single_segment_response(segment, ai_analysis, idx)
                    scored_segments.append(scored_segment)
                    
                    # Add delay between requests
                    if idx < len(segments_to_score) - 1:
                        time.sleep(inter_request_delay)
                    
                except Exception as e:
                    logger.error(f"Failed to score segment {idx + 1}: {str(e)}")
                    scored_segments.append(create_fallback_segment(segment))
            
            else:
                # Batch processing for multiple segments
                try:
                    # Create batch prompt
                    batch_prompt = create_batch_prompt(batch_segments, prompt_template)
                    
                    # Call AI model with retry logic
                    ai_response = None
                    api_call_succeeded = False
                    max_retries = 1
                    
                    for attempt in range(max_retries + 1):
                        try:
                            if AZURE_SDK_AVAILABLE and isinstance(client, ChatCompletionsClient):
                                response = client.complete(
                                    messages=[
                                        {"role": "system", "content": system_message},
                                        {"role": "user", "content": batch_prompt}
                                    ],
                                    model=model_name,
                                    temperature=temperature,
                                    max_tokens=4096,  # Larger for batch responses
                                    response_format={"type": "json_object"}
                                )
                                
                                ai_response = response.choices[0].message.content
                                
                                # Track token usage
                                if hasattr(response, 'usage'):
                                    tokens_used += response.usage.total_tokens
                                
                            else:  # OpenAI SDK
                                response = client.chat.completions.create(
                                    model=model_name,
                                    messages=[
                                        {"role": "system", "content": system_message},
                                        {"role": "user", "content": batch_prompt}
                                    ],
                                    temperature=temperature,
                                    max_tokens=4096,  # Larger for batch responses
                                    response_format={"type": "json_object"}
                                )
                                
                                ai_response = response.choices[0].message.content
                                
                                # Track token usage
                                if hasattr(response, 'usage'):
                                    tokens_used += response.usage.total_tokens
                            
                            # API call succeeded
                            api_call_succeeded = True
                            api_calls_made += 1
                            logger.debug(f"Batch {(batch_start // effective_batch_size) + 1}: scored {len(batch_segments)} segments with API")
                            logger.info(f"API call {api_calls_made}: {len(batch_segments)} segments, {tokens_used} tokens used so far")
                            
                            # Add inter-request delay
                            if batch_start > 0:
                                time.sleep(inter_request_delay)
                            
                            break
                            
                        except Exception as api_error:
                            # Check if it's a 429 rate limit error
                            if hasattr(api_error, 'status_code') and api_error.status_code == 429:
                                retry_after = extract_retry_after(api_error)
                                if retry_after and retry_after > max_cooldown_threshold:
                                    logger.error(f"Rate limit exceeded with long cooldown ({retry_after}s). Stopping scoring.")
                                    remaining = segments_to_score[batch_start:]
                                    save_pipeline_state(scored_segments, remaining)
                                    return scored_segments
                                
                                sleep_time = (retry_after or 10) + random.uniform(1, 5)
                                logger.warning(f"Rate limited. Sleeping for {sleep_time:.1f}s")
                                time.sleep(sleep_time)
                                continue
                            
                            if attempt < max_retries:
                                backoff = min(300, (2 ** attempt) + random.uniform(0, 1))
                                logger.warning(f"API call failed for batch starting at {batch_start + 1}, retrying in {backoff:.1f}s... ({api_error})")
                                time.sleep(backoff)
                            else:
                                logger.error(f"API call failed for batch after {max_retries + 1} attempts: {api_error}")
                                raise
                    
                    if not api_call_succeeded or ai_response is None:
                        raise RuntimeError("API call failed and no response received")
                    
                    # Parse batch response
                    batch_analysis = extract_json_safe(ai_response)
                    
                    # Extract results array
                    results = batch_analysis.get('results', [])
                    
                    if not results:
                        logger.warning("Batch response missing 'results' array, falling back to individual parsing")
                        # Fallback: try to treat as single response
                        results = [batch_analysis]
                    
                    # Process each result and match to original segments
                    for i, segment in enumerate(batch_segments):
                        idx = batch_start + i
                        
                        # Find matching result by id
                        result = None
                        for r in results:
                            if r.get('id') == i + 1:
                                result = r
                                break
                        
                        if result is None and i < len(results):
                            # Fallback: use positional matching
                            result = results[i]
                        
                        if result:
                            scored_segment = process_single_segment_response(segment, result, idx)
                            scored_segments.append(scored_segment)
                        else:
                            logger.warning(f"No result found for segment {idx + 1} in batch")
                            scored_segments.append(create_fallback_segment(segment))
                    
                except Exception as e:
                    logger.error(f"Failed to score batch starting at {batch_start + 1}: {str(e)}")
                    # Add fallback for all segments in batch
                    for i, segment in enumerate(batch_segments):
                        scored_segments.append(create_fallback_segment(segment))
    
    # Sort by overall score (highest first)
    scored_segments.sort(key=lambda x: x.get("overall_score", 0), reverse=True)
    
    logger.info(f"Completed scoring {len(scored_segments)} segments")
    
    if scored_segments:
        # Calculate statistics
        scores = [s.get("overall_score", 0.0) for s in scored_segments]
        final_scores = [s.get("final_score", 0.0) for s in scored_segments]
        max_score = max(scores)
        avg_score = sum(scores) / len(scores) if scores else 0
        max_final = max(final_scores)
        avg_final = sum(final_scores) / len(final_scores) if final_scores else 0
        
        # Get threshold from config
        min_score_threshold = model_config.get('min_score_threshold', 6.0)
        
        # Count high-quality segments
        high_quality = sum(1 for s in scores if s >= min_score_threshold)
        
        logger.info(f"Scoring complete: max={max_score:.1f}/10 (final={max_final:.1f}/100), avg={avg_score:.1f}/10 (final={avg_final:.1f}/100)")
        logger.info(f"High-quality segments (score >= {min_score_threshold}): {high_quality}")
        
        # Sanity check: warn if ALL segments scored 0
        if max_score == 0 and max_final == 0:
            logger.warning("=" * 80)
            logger.warning("WARNING: ALL SEGMENTS SCORED 0!")
            logger.warning("This likely indicates an API or parsing issue:")
            logger.warning("  1. Check if API key is valid and has correct permissions")
            logger.warning("  2. Check if model endpoint is accessible")
            logger.warning("  3. Review raw model output logs above for errors")
            logger.warning("  4. Verify response_format is supported by the model")
            logger.warning("=" * 80)
    else:
        logger.warning("No segments scored successfully")
    
    return scored_segments
